1. 我的分布式文件系统中共有三类角色，类似Fast-DFS，分别是运行在用户主机上的client，是整个系统的入口；tracker节点作为调度服务器，负责调度所有存储服务器；storage存储服务器，真实存储文件的服务器
2. 每类角色在系统的架构是：tracker一台，storages多台，storages分为多组，每组三台storage服务器。
3. 大致流程是：当client获取到用户的文件后，按照4MB为一单位将大文件分成多个slice，请求tracker查找合适的组，tracker内维护者当前每组storage的状态，从中选出空闲较多的一组/多组，将信息返回给client，client依次完成传输

整个系统真的很大，很多解决方案都很细节

# 1. 单个4MB文件如何实现修改
- 我们面临的问题是：如果改小了（也就是删除某些部分）只需要重新替代即可，如果改大呢？超过4MB的数据必然需要写入新的slice
  那怎么把这个新分片插入到原本的链表？这其实也是用链表的原因之一：
  解决方案是：在每个slice中头部存在元数据（6bytes），2byte存储下一个分片的位置（storage编号 + 偏移量index），4byte存储此slice分片中有效信息占用的长度
  这个操作主要关注第一部分。
  并且由于是链表，当修改时试图将新分片写入响应storage代替旧的时，避免在此从链表头部开始找，在client中会短时间存储每个节点的位置（就存储在上一个分片的元数据里）
  所以这样就很简单了：只需要改变原本slice指向下一个slice的指针，其中插入任意新分片都是可行的。


# 2. 系统如何实现分布式事务
  - 我们要解决的问题是：无论是在上传还是更改过程中：当用户将一个大于4MB的文件交给client，client首先将大文件分成n个4MB的slice，请求到tracker之后，tracker返回m个可用节点，
    client依次向每组storage发送分片信息；假如一组中写入失败，要想办法将这个操作原子化
    即：要么坚持让这组写入成功 / 要么全部回滚
    - 第一个策略的解决方案很简单，一组存储服务器存储失败，超时后也没返回确认信息；此时需要区分是网络问题还是真实宕机
      > 系统设计中大概率是网络问题;因为系统的每个storage存储服务器都有两个副本，leader宕机后通过Raft共识算法很快会再选出新leader来，所以后面的操作不考虑一组服务器全部宕机
        网络问题的解决方案就很简单：尝试接着重发，直到收到响应；

    - `client请求tracker分配另一台，直到将这个分片成功存储
      但是并不适合我们项目架构 - 原因是我的每个分片是通过链表联通的，在离开client后更改位置的代价其实很大`
      
    - 第二个策略比较复杂，因为它需要考虑文件系统怎么实现回滚操作
      文件系统和数据库系统不同，文件系统如何要回滚就需要在提交前同时保留老数据和新数据，并且提交这个操作也可能是需要写文件、而不是向数据库一样改变指针
      我的解决方案是这样的：本质还是2PC，由client负责协调
    - 在预提交阶段，当storage准备写入1个4MB时，首先写到整个节点的最后，假如确认提交，将原本指向老分片的指针指向新的分片，假如回滚则把原来表示写入位置的指针前移4MB
      第一个问题是：怎么更改原本指向老片区的指针？
      - 因为我的整个大文件是链式的，上一个分片和当前分片有可能存在一个节点也有可能不是，假如通过网络请求找到上一个分片再改信息，这个操作时间太长了，如果把这个操作加入整个事务很安全但损失太大
        所以我用类似重定向的方式解决 - 原本上一个分片任然指向老区，老区有一个重定向信号，标记着新区的位置。
      第二个问题是：如此一来，每个分片的上传/更改一旦确认就会产生新的无效区域，这个无效区域还没很好的办法重新利用（因为存储着重定向信息，如果想要保留重定向信息的同时写入文件是很困难的，既要让client在分片时给这个分片空出一定的元数据位置，同时如果这个新写入的分片再次需要重定向，那么要存储的重定向信息是越来越大的）
      - 解决方案是这样的，因为如果是被删除的分片，其实是可用重新利用的，但重定向的分片不行
        我用一个空白区域表记录这些分片，对于被删除的分片会在某个时刻被上传的文件重新占用，对于重定向的文件一旦发现其大小太大，会触发一次下线整改（这个还在写）

    > 暂时在上传时使用第一个，跨节点修改时使用第二个
    > 第一个只需要增强一些确认策略就可以保证比较强的一致性
      但第二个策略扩展性更强，一致性也更强（因为2PC本身就保证回滚和提交两状态只需要一个commit指令），第一个中补偿过程可能很漫长，中间如果client或者tracker出现问题可能有风险
      扩展性更强 --> 实现了回滚逻辑 --> 基于这个回滚可以提供更多功能 --> 2PC中间提供检查功能；跨事务回滚（提交后能撤回）；
      
    
# 3. 如何解决并发写的问题：
  - 刚刚说到引入了线程池，线程池在系统中的作用不光是提高并发，其实还能监控并发程度
    关于提高并发我是这样使用的：因为对于同一文件的修改并不需要整体加锁，即（大文件级别的锁），其实对于修改的操作，只需要保证对于同一个slice的修改操作能串行即可
    所以我在内存中维护了n把锁（细粒度锁），每个分片对应一把锁，这样虽然并发线程同时进入线程池，但是某些线程可能阻塞
    细节的实现是：没把锁其实是通过偏移量绑定的分片，也就是说当修改分片时先在锁列表中取出对应锁来加锁
    问题是：每个分片的位置经常变化，锁如果还根据偏移量绑定slice的话，获取锁的时候就容易对应不到正确的锁
    解决方案：架构了类似**锁池**的概念：Hashmap - key是头部元数据的，value就是锁，因为元数据基本不变，所以slice和锁的绑定关系也就确定了


# 4. 将Raft算法初步结合到项目中：
  - 对于Raft算法的代码实现比较简单，只实现了简单的领导者选举和日志复制，很多出于安全考虑的预防策略还都没有实现
  - 选举的实现比较简单，按照Raft算法实现就好，但日志复制的逻辑要修改
    - 日志复制：
      - 原本对数据的指令都在日志中，follower依次读取日志的操作在自身上就完成同步，但这个过程比较适合与数据库层面的操作 -- 每个日志中数据量较小
        但我的文件系统是以4MB为一个单位，假如将数据写在日志中发给follower，follower中维护的消息队列将巨大无比
      - 给出的解决方案是：指令和数据分离 -- follower得到的日志中只保存要向leader申请的片区的信息，通过异步的线程池来处理这些日志（更像一个本地消息队列）
        问题1：原本leader收到client传来的4MB，在内存中加载一次就可以淘汰，但现在让follower反向的请求leader，会导致leader加载多次片区信息
        解决方案是：
        - 初步是先加个缓存，4MB的数据在leader提交后还是先缓存起来，当两个follower都申请完后淘汰掉（或者超时）
          实操中就是上文提到的登记表，这就是value中任然需要整个数据的原因
          登记表的作用：HashMap<String,Slice>，Slice中有data，checkCount【表示两个follower收到请求后发回确认信息的数量统计，到达2时entry从map中淘汰】
        第二个预期的解决方案是：
        - Netty / NIO中的零拷贝技术，直接能将数据从磁盘到操作系统，直接到达Socket再到达网卡，不经过内存中。


# 5. tracker，storage的Leader持久化
- 策略1：
  - 思路参考HDFS，设置核心镜像文件fsImage和补偿的日志文件Edit Log，但是也根据我的项目特点进行了修正
    具体是这样：以持久化tracker内存中的数据为例：按HDFS的思路是将存储文件元数据的concurrentHashmap每一小时持久化一次，在其中间隔的一小时内的操作记录到Edit Log中，
    宕机/下线后重启就只需要先读取fsImage，再把EditLog内的指令依次执行完即可
  > 这种思路比较适合对元数据修改很频繁的操作
- 策略2：
  - 我的项目进行的调整是：因为架构的时候tracker的元数据只是每个大文件的头部slice嘛，并没有记录所有分片的位置，也就是说对tracker内元数据的修改只有上传文件时；
    所以又从rabbitMQ中参考了一个技术【惰性检查】：
    即：对tracker的持久化并不经常发生，而是嵌套在文件上传的整个事务中 -- 适合文件上传并不频繁的场景【例如测试场景】
    具体而言：文件在client端分片后请求tracker，tracker返回可用storage节点，concurrentHashmap中添加头部节点以及所在位置和偏移量，
            client作为协调者和多个storage完成两阶段提交，提交完毕再次请求tracker确认
      假如中间出现问题：client负责下达回滚指令【手动事务操作】
    > 其中应该尽可能减轻事务中的操作，把能解耦的解耦 —— 但异步解耦的条件是能够确保可靠性


---
分割线

简易分支 - 基于Windows文件系统
 > client将slice交给storage之后，storage并排将每个4MB存储成一个文件，底层到底是怎么管理这么多4MB数据不得而知
   并且对其中一个slice的修改扩大操作直接调用操作系统的API即可完成，底层实现了在本地选取新空间并建立逻辑上的索引
    
   两个分支间的关系：
   > 做到对上层的解耦 --> 即对上层提供同样的API（实现同样的接口）
     API包括：
       1. slice上传 (并发写 ——> 补偿)
       2. slice修改 (改大 ——> 回滚)
       3. 查找slice (根据名称？根据偏移量？)
所以接下来要把系统解释清楚就要分两个部分：
   1. 基于操作系统层面文件系统的分布式架构    
   2. 适合此场景的文件系统 

- 改操作：
  - > 出于性能考虑，避免直接使用windows文件管理，
      而是手写操作系统层面的文件系统 -- 实现比较简单，还没有考虑加入目录/文件夹，现阶段就是以数组的形式将所有文件一字排开
      底层的存储、追加写、修改的大致逻辑和许多单机文件系统类似，(意味着还没有兼容其他系统的优点)
  - 跨平台？ -- 原本Java是跨的，但是Java代码不能直接修改磁盘空间，所以有一个想法 ———— 编写C的操作磁盘的API，Java通过JNI引用C的轮子 ———— 更快
            -- C本身不跨平台，那就只能写多个C的代码（驱动），让Java跨平台
       > 不同文件系统的文件结构安排
        1. 层次目录结构 -- 树状结构
        2. 哈希目录结构 -- 哈希目录结构
        3. 平坦目录结构 -- 所有文件都在同一级目录下，没有子目录
  - > 参考了FAT32，Inode，都只适合单机的文件系统，具体单机内的文件分区大同小异
        将分布式集群中的Inode抽离成一台服务器，在此服务器内完成分布式层面的调度 —— fastDFS的tracker
        但我的链表更进一步
    - 追加写和直接改：
      - 追加写：判断大小是否小于最后一个分片的空余部分，如果不小于就直接在新分片中写
        - 也要处理并发写问题，并发写的内容虽然只需要前后排开，但需要考虑并发写后是否超过服务器最大空间
            处理方式：写时复制 | 但需要让下一个用户知道还有没有足够大的空间来写 - 变量temp记录排队人数，计算并发即将占用的空间
          - 什么时候合并归一 —— 当线程池中可用线程多起来的时候，并且这个判断写在上传文件的方法中，这样做在没有并发时会一个一个稳定上传，在有并发时会在后面几个触发
            要注意在归一的时候给暂存list和排队数加锁
      - 直接改：
        > java的File读取是基于Stream的，但提供了RandomAccessFile可以直接操作底层
        - 在一个slice内的修改
          - 对于存储满的分片改大必然会溢出，改小会在末尾空出空间
            - 解决方法1：标记追加写方法 —— 追加写，但内容是改某一部分  |  解决新的问题——追加写少内容太多次，导致空余内容太多，定期垃圾回收
            - 解决方法2：链式
              对于更改文件且超过原本空间的：
              将newFIle前4MB写入一个Slice，且设置next为自身，剩下每4MB的存入下一个slice，且设置next为刚刚的next
            - > 具体实现是：整个大文件转化成inputStream流，前(4MB-2bit)读取并存储完后，接着读(4MB-6bit),只有当读满(4MB-6bit)并且输入流中还有数据时，将此
                slice的next设置为自身，nextIndex设置为下一个，否则说明此slice就是整个文件的最后一个
            - --- 问题：怎么优雅的读出4MB-6byte？
                  我只能不优雅的先循环读取4MB-1024Byte，再读一次1018Byte
            - --- 问题：需要明确在storage中怎么找到对应的slice --- 根据hash在List<String>中找--类似索引
            - 并发改怎么办？
                - 在每个storage（JVM）上我给每个slice分配了一把锁，并发写同一个slice的操作自然排队
                  而分布式锁使用的场景是：多个线程修改一个资源，通过在第三方管理中争夺锁（redisson,zookeeper）来获取权限，但我的程序中storage本身就算第三方

      - ```问题记录：怎么保证修改过程中不触及元数据？
        
        ```
        - 跨越多个slice的修改 - 就是跨越多个storage的修改
          分布式事务
          - 思路1：不考虑分布式事务的原子性，就算某一部分写入失败，其他部分即使写成功也不撤回 --> 对于修改操作不行
            具体：更改某个slice位置，真必须要该上一个片区的头部(无语死啦)
            - 思路2：配置一台控制服务器，用于2PC，3PC监控提交 --> 
            > 直接改的流程是：client通过tracker查询到目标文件的位置，分别查出来由client拼接成大文件后呈现给用户，用户分别修改每个slice，将修改的部分提交回去
              storage将修改的slice替换原来的slice，并且有可能扩展几个slice
            - 很明显client作为协调者，监控多个文件的替换是否成功 -- 2PC
              - 问题：写入文件怎么能 执行但不提交？
                1. 思路1：充分利用每个slice的validIndex —— 新的要替代的数据在末尾正常写入，假如commit则将原本的slice无效化（validIndex = 0），
                   末尾的新slice代替旧slice，假如要回滚则把新slice的validIndex设置为0； 
                   **好处是**：把宕机的风险压缩到切换一个指针的时间内，和数据库中commit功效类似，下面的写时复制就无法做到这样
                   **问题在于**：要解决storage中间和后面validIndex == 0的slice，后面好说直接改startIndex，中间的只能放弃吗？
                   又一问题在于：要改旧slice上一个slice的头部信息，要求slice的元数据里要有上一个的location和index
                   又一问题在于：改动上一个slice的头部信息也涉及分布式事务，也需要client调度
                   - 差强人意的解决方法：通过MQ异步通知上一个节点，调整节点头部的元数据 (问题在于太不保险了)
                   - 终极解决方案：类似 **重定向** ，在哪个空白的位置记录301，当client访问到301时，重新由client发起对新地址的访问
                   又一问题在于：假如修改范围包括第一片slice，那tracker中对于链式的头节点的记录就要更新了，client缓存也要更新
                   所以更新client缓存的时间是：
                   1. 上传成功
                   2. 修改涉及头节点（怎么区分谁是头节点？元数据中上一个节点location和index为-1）
                   
                    解决写时复制会导致无效空间的策略：
                      1. 修改写入新文件的策略 - 先检查前面有没有空白？
                      2. 有些文件系统的策略是 优化写时复制策略 - 尽量在原数据块中追加修改，而不直接该原数据块
                      3. 空白区域表，（4 * n MB），由于每次上传都加锁访问空白表会造成造成性能瓶颈，所以只能在线程池中空闲线程较多（并发不高）时填充碎片
                    
                    新版本中，无效空间中存在有效数据，不敢轻举妄动
                    设置空白地区状态码协议： 1byte
                    ```
                    {
                       404 （表示被删除）
                       301 （表示被重定向）
                    }
                    ```
                   最终策略：空白区域表中，记录的位置都是被删除的空白slice，不能是301的slice                    
                   问题：因为修改而产生的空白区域太多了，不做碎片整理有点不合理

                2. 思路2：MVCC
                3. @Transactional注解
                  这个事务管理是基于数据库的，有时还要绑定dataSource，并不能解决文件写回滚问题
                4. 还得是 | 写时复制 |
                 相比于第一种思路，慢在选择commit后还要等待复制，但并不会产生莫名的无效空间
                 ```有问题：
                 思路：对于大于4MB的文件，先只把4MB-6byte后面的部分写完；前面部分等待协调者的指令，如果commit，复制副本进入主体，如果rollback，调回startIndex
                 问题：commit的时间太长了，在commit中发生宕机的概率很大，导致2PC没有意义
                 ```
                 应该在prepare阶段为commit失败的风险兜底 -- 还得是先写到副本里，如果宕机的话就不会清理写到副本里的，于是重启就可以用于恢复
                 ```
                 思路：对于大于4MB的文件，先执行事务操作 -- 将4MB-6byte后面的部分写完，同样也需要将4MB-6byte的数据持久化了（跟在最后面），
                 如果commit，将在最后面的slice复制进入主体，startIndex前移屏蔽掉它，如果rollback，startIndex前移屏蔽掉所有
                 对于小于4MB的文件，也是先在最后持久化写完
                 ```
                 其中使用到的技术：子线程对外传递结果 -- 异步任务提交 -- future类 + callable接口
                 但宕机重启后读取文件恢复的程序没有写
          - 删除功能
            - 也会产生空白4MB，那就伴随着写时复制产生的空白，一起清理吧
                   

- storage节点文件系统安排
  第一个4MB：保存超级元数据、运行在内存中需要持久化的部分（设置启动后自动读取并执行）

     - 分片大小设置：
       - 暂不明
     - **分片结构设置：**
         1. 有下一个分片的位置 1byte
         2. 有下一个分片的index（所在storage的具体偏移量）1byte //取消
         3. 有有效信息占分片的长度 4byte
       整体 6byte
        > 两个都由上层tracker做吧
  - 所以在哪里将大文件分片并且添加标头？数据不经过tracker，就在client
    > client将大文件分成4*1024*1024-6，并为每个slice添加标头----需要在请求tracker之后得到目标storages和对应的index
      然后分别将slices分配到各个storage的leader，leader再按照raft完成一致性
      过程中，tracker中有每个slice的storage和index，|以及每个大文件对应的slices| ~~~~ 
      很矛盾的一点是：tracker要避免对文件的任何操作--计算哈希/分片都交给client，但tracker又需要分片对应关系+哈希
      在向tracker找文件时，正是通过hash找到头节点
    - tracker怎么知道哪些storages是空余的，每个storage下一个存到哪里？
    - storage每隔一定时间发起心跳证明自己存活，在心跳中传递必要信息：storageId，startIndex
      心跳信息：
      ```JSON
      {
        storageId:**,
        startIndex:**,
      }
    - ```
    - 怎么检测storage是否宕机？消息队列的设计是：生产者一发送，生产者就可以被动接受（监听器模式），这样可以按照心跳的频率更新storage的上次心跳时间
      但是遍历检查storage有无心跳（上次心跳时间是否太久远）需要轮询吗？
      方案1：每次接受到心跳时遍历一遍，只要有一台机器存活就可以检测出其他机器宕机
      方案2：**又一线程轮询（和心跳同频，稍慢）**
    - tracker持久化文件信息（内存镜像文件） 文件对应关系
    - > 替代tracker持久化的方式：记录编辑日志 —— 实时写入每个修改文件的操作（事务操作），无论tracker何时宕机，甚至不需要内存镜像文件也能恢复
        但只能 | 配合使用 | --> 镜像文件每一个月更新一次，编辑日志记录此一个月的编辑过程
    - client 缓存tracker中文件对应关系，但当对应关系变动时触发刷新 --- 会不会刷新太频繁？
- 又一思路：链表化 / tracker只存储头节点的位置，接下来每个节点都内含下一个节点所在位置
  - 解决了直接改的弊端：可以直接在后面加一个4MB
  - client的缓存刷新也不必太过频繁
  - 链表化的话，垃圾回收是必要的
  - 缺点：
    - 改操作时怎么能直接修改更改的那部分？难道还要从头根据偏移量找？ | 解决方法是：更改时通过链式找到slice时，直接记录每个分片的位置
    - 每个slice的元数据听起来好用，实现起来略显复杂
      1. 存储服务器对修改后的大文件负责分片+添加头部信息
      2. 确保头部信息不会被修改 -- 在前端修改过程中要映射好修改的文件和实际存储文件的关系 | 前端的文件由一系列不到4MB的slice拼起来...
- 对于tracker
  - 测试元数据大小----tracker压力
  - 信息都在内存中，这就导致宕机危害很大，策略是**优先记录日志，其次更改内存** -- 但我的DFS未必需要这样 -- metaData的数据不常更改且备份在client的缓存中，宕机也无妨
    - 最极端的情况：client向tracker发起登记信息，没登记完tracker宕机 -- 也无妨，用消息确认（返回true）保证成功登记
  - 策略是：client先向tracker请求到一系列可用的storage序号，提交成功后，再向tracker发起记录信息

  - 元数据内容配置：
    1. 文件名称
    2. 整个大文件的第一个slice所在storage，以及偏移量index -- 由于链表化后会在中间增加slice，所以没有存下每个slice来
    潜在配置：文件大小，大文件哈希...但构建的思路是元数据尽量少修改，所以长变的不计入
    ```bytes
    ConcurrentHashMap<Sting,Integer[2]> -- 表示文件名 对应 头节点所在storage+index
    ```
  - 持久化：暂无
    

- 解决并发写的问题：
  - leader会有同时写的风险，解决方法暂定 —— 细粒度锁
  ```以下策略没能落实，用 n 把锁解决问题
  - 乐观锁：
    - 当写的多方之间并没有重叠，最后能合并即可提交
    - 但如果有重叠，选择某个重叠的用户提交失败重新编辑吧
  - 具体实现：
    - 创建一个map，key表示修改的slice的index，value表示具体的file
    - 当初次添加某个key时可以正常操作，后续添加会检查是否containsKey，如果contains，标记失败返回false即可
  ```
  
> 我怎么知道谁和谁是并发的访问？
  哪两个线程间需要考虑并发？ - 是会使用公共变量的线程 -> 只需要对这些线程上加锁
  原本写时复制的构想是：拦截下来一段时间内并发访问的线程，将这些线程中，后面重复的部分返回修改失败，前面的部分返回修改成功
  问题就是：很难显式的得到并发下的所有请求线程，应该是将算在一个并发的时间范围设置成执行方法的时间

> 如果内存中显式的维护n把锁呢？

- 可视化架构：
    - 核心：ManagementFactory
    - 需求：
      1. 获取实时对storage的访问量（通过获取线程池的线程数）[总 / 实时]
         - 总访问量：表示此storage的类对象中维护一个计数器，通过拦截器进行统计
         - 实时访问量：统计上传文件的线程池、下载文件的线程池的可用线程数
      2. 记录每个slice被访问的次数（1. 持久化 2. 内存中维护hashmap，被访问的方法中执行+1）
         - 也放在拦截器中，从request参数中获取到目标地址，然后在内存中维护的Hashmap中执行+1
         1. 持久化：ObjectInputStream + ObjectOutputStream
         2. 维护了两个hashmap，一张是slice的总访问数，一张是slice的本次启动访问数
      3. 热点化 -- 简单起见将热点数据存储在本地，根据本次启动访问数表将访问数量多与20的byte[]存入内存 | 
         具体而言，之前次数少时只会访问磁盘，并给两张表都+1，第二张表中每个slice达到20就添加到内存
      4. 当前QPS
      5. 关于JVM的【[-GC-]】等，Management都可以监控，通过JMX提供的MBean和MXBean
    - 二号核心： Metrics
      - 由于Metrics导出类型很丰富，console,csv,JMX,http
      > 细说HTTP：通过JSON返回信息
        ```
      <dependency>
        <groupId>io.dropwizard.metrics</groupId>
        <artifactId>metrics-servlets</artifactId>
        <version>${metrics.version}</version>
      </dependency>
         ```
      有一种想法是：减少所有监控线程/方法的执行复杂度，能最简单的输出日志是最好
      策略：
        1. 后台主动推 -- 每秒将数据传到消息队列或者redis中，前端任意时刻获取
           > 使用metrics的httpReporter似乎只能由后往前推（绑定前端的url，由前端解析请求头的内容） 
        2. 前端触发检测方法执行
           > 使用managementFactory 可以实现

           
## Client！
  1. 缓存怎么设置：
    由于存储的内容是简单的文件名 + 文件头部slice所在位置(storage,index) --> ConcurrentHashmap
    不强制要求每隔时间刷新缓存，而是在头部变更时，执行缓存刷新
    


## 核心问题记录：
  - 将文件分片依次存储不同storage上太分散了，白白增加了节点宕机影响全局的可能性
    应该尽量将文件存储的紧凑一点，首先满足负载均衡（不会将全部文件挤到一起），其次是相邻的尽量存储在一起
    > 假如规定最多每3个slice可以连续存储呢？对于小文件任然是一个slice不会占用太大空间，对于稍大文件减缓了分片过于分散的问题
  - 所以存储策略修改为：
    - tracker有client发来的片数请求，本身监控每个storage可用区域，如果片数少于等于3，存储在剩余空间最多的storage上，如果片数等于7，分配为3+3+1...
      并且配置信息3可以因存储文件属性不同而进行修改，使得myDFS能够适应更多存储环境
    上面的一切架构不会都要改吧
    - 偷懒日记：
        - 存到哪里和storage无关，所以只要tracker返回给client的目的服务器中表现出需要连续存储，client给storage的指令是基于片的，也可以架构出连续片 
        - 连续？为什么连续？只要在一个storage内不就行了？这样锁的粒度也不需要加到一个线程，而是一个slice
    - 并发写：
        - 上传只需要解决并发请求依次排开，超过storage界限的取消存储即可
        - 怎么能保证请求依次排开并且不会导致阻塞？ -- 每个线程都根据更新后的startIndex直接写入
        
      
## 回来吧我的Raft，我最骄傲的信仰
  1. 选举：
     - 同组内所有服务器启动时角色都是follower，有一个线程记录随机的超时时间，某一时刻一个节点超时了，成为candidate
     - candidate将自身term + 1，向所有follower发起 vote RPC，表示申请投票
     - 过程中可能有多个candidate对外发起请求，一个follower只能投一票（我的项目中，同组只有三台，只要三个的随即超时时间别太紧密，第一个都会得到两票）
     - 收到两票的candidate成为leader，停止超时时间线程，又启一个线程定时发起心跳
  2. 日志复制： 
     - Leader收到文件后，用线程池内两个线程将slice发向follower，follower写入成功后返回确认信息，leader收到一个确认信息后自身完成commit
     - 同时leader内存中运行着一个登记表，HashMap<slice的哈希，boolean[true,false]>,定时扫描哈希表，对false的follower完成消息补充

     - 问题在于，没法执行AppendEntries，AppendEntr ies适合数据库类型，可以通过追加条目完成对之前内容的修改
       - 思路 1:本地消息队列 -- 指令和数据分离
           follower中维护着Queue，用于获取到AppendEntries信息，这一层完成Raft的一致性算法
           核心线程池获取到Queue中的AppendEntries条目，异步的完成向leader请求片区信息
           问题：
           - 原本从leader向follower发，只需要将slice读到内存一次即可，这种方式发需要读取2次IO
           解决方案：
           - Netty 零拷贝技术
           - 刚到的文件加缓存 + HashMap中维护着的两次请求结束后淘汰（或者设置最长淘汰时间）（或者在生产环境中估计出一个淘汰时间）
       - 思路 2: 日志直接加载slice数据
           问题：
           - follower中需要维护多个日志，内存中开销太大了

## 系统下一步发展
  - 文件结构的组织和检索（多层的结构，在client做逻辑的分层和组织） -- INodeDirectory + InodeFile 【文件结构构建出树，访问子文件就是向下遍历树的过程】
  - FTP代替HTTP
  - 系统的可扩展性 （动态下线、动态上线）
  - 真实解决很多分区问题、节点故障、恢复后读取备份...容错问题
  - 开源
    
